
\documentclass[Rapport]{subfiles}
\begin{document}

\chapter{Optimise}
\label{sec:Optimise}

% svårt att låta bli att alla dessa avsnitt börjar med 
% 'i det här avsnittet [ ska vi undersöka / beskrivs / förklaras ... ]'
% plepp, pleat

\overviewOptimise

Nu när sockerspråket (kapitel \ref{sec:Socker}) och STG (kapitel \ref{sec:STG}) är förklarade
kan undersökningen av optimeringen, som är själva kärnan i arbetet, påbörjas.
I detta avsnitts introduktion förklaras vad optimering är och 
vad det innebär att göra det under körningstid. 

För att optimera under körningstid har olika lösningar, som följer olika semantiker, tagits fram och undersökts.   
Den första av dem, som kommer att refereras till som CBV- eller call-by-value-semantiken, 
beskrivs i avsnitt \ref{sec:Optimise:CBV}. Den har vissa nackdelar, så i 
efterföljande avsnitt presenteras en semantik, som kallas för CBN- eller call-by-name-semantiken, för en bättre lösning .

Lösningen tjänar också på andra optimeringar, som t.ex.
dödkodseliminering (sektion \ref{sec:DeadCode}), 
välkänd caselag (avsnitt \ref{sec:CaseLaw}) och 
afterburner (avsnitt \ref{sec:Afterburner}).
Hur optimeringen fungerar tillsammans med anropsstacken visas i avsnittet, \ref{sec:CBN:Callstack}.

\subfile{Optimise/Intro}

%\subsection{Optimeringsmaskiner}
\begin{comment}
Statiska optimeringar är ofta uppdelade i olika pass, specialiserade på en viss
typ av optimering\footnote{Vi kan se varje pass som transformen $\llbracket \_ \rrbracket :: syntaxtree \rightarrow syntaxtree$}. Det kan ibland finnas anledning att köra ett pass flera gånger. Till exempel 
kan ett tidigare pass ha skapat nya optimeringsmöjligheter för ett annat, redan kört, pass. 
Att köra dessa pass under kompileringstid är inget större problem, 
men under körningstid finns det större krav på tidseffektivitet, vilket denna iterativa 
metod har svårt att leverera.

Vi valde istället att skriva en ny del av evalueringsmaskinen som tar över när något skall optimeras.
Denna har precis som STG ett antal regler som den följer beroende på vilket
tillstånd den har. Till skillnad från STG tillåts variabler att vara okända när optimeringen
körs, eftersom den är tänkt att köras på partiellt applicerade funktioner eller 
evaluera under lambdat som det också kan kallas.
Precis som STG använder den sig av continuations, vilka används för att bygga upp 
syntaxträdet när optimeringen är klar. Med det här tillvägagångsättet behöver vi 
bara gå igenom syntaxträdet en gång. Vi har delat upp optimeringen i tre tillstånd
som har olika ansvarsområden. Optimeringen kan också gå tillbaka till STG-maskinen
och använda den för att evaluera uttryck, vilket gör att vi slipper duplicera
den funktionaliteten.

%\stgOptimise


\textbf{Omega} $\Omega$, är ingångspunkten till optimeringsfunktionen, och anropas från maskinen
när något har blivit annoterat för optimering. Omega bryter ner trädet
och delegerar vidare vem och vad som ska ske, samt lägger ut continuations
för att senare bygga upp ett (förhoppningsvis) nytt träd.

\textbf{Psi} $\Psi$, anropas när optimise har lagt ut något åt STG-maskinen och
den inte kan komma längre. Psi kommer att använda värdet från maskinen
och sedan delegera vidare optimeringen beroende på det.

\textbf{Irr} $\Phi$, kortform för Irreducible. Anropas när Omega eller Psi inte
kan optimera vidare. Irr kan då utifrån vilka continuations som ligger på stacken
bygga upp trädet eller byta continuation och fortsätta optimera på en
annan del i uttrycket. Det är också här vi till slut går tillbaka till
STG-maskinen.

Vi har använt ett par olika semantiker för att beskriva optimeringen
i vårt språk, och vi ska nu förklara dessa i kronologisk ordning.

\end{comment}

\section{Optimeringtillstånd}

% exotiskt att skriva dessa i optimisefilen
% och inte i en ny fil i optimisekatalogen
%  mvh Dan

Optimeringen kommer att uttryckas på ett sätt som liknar semantiken för STG. Koden optimeras
 genom att den evalueras på ett speciellt sätt. En skillnad mellan optimeringsevalueringen och en vanlig
evaluering är att optimeringen sker med okända uttryck (t.ex. fria variabler), 
vilket gör att det inte alltid går att få ett värde\footnote{ Det vill säga en primitiv eller en variabel som inte
är en thunk.}  när ett uttryck optimeras. I dessa
fall måste ett nytt uttryck byggas upp som kommer att räkna ut det okända värdet. 
Förhoppningen är att det nya, optimerade uttrycket, är mer effektivt än det gamla.

Vi ser optimeringen som ett speciellt tillstånd som STG-maskinen befinna sig i. 
Detta tillstånd används för att maskinen ska veta att den evaluerar inuti en funktion
(under lambda), vilket alltså medför att det finns fria variabler. 
Målet är att optimeringen till slut ska ha skapat en ny funktion som är snabbare 
än originalfunktionen.

Det som optimeringen kan göra faller under dessa kategorier:

\paragraph{ Beräkning av kända uttryck }
Dessa fall är ganska enkla att optimera, eftersom det går att använda den vanliga
maskinen för STG. Om det t.ex. står \ic{7 * 191} i koden så kan det evalueras
som vanligt och resultatet \ic{1337} kan sedan användas i koden.

\paragraph{ Infogning av funktionsdefinitioner }

Att infoga funktioner kan ses som något enkelt, men det finns vissa fällor att se upp
för. Det gäller att se till att funktioner som i sin tur infogar funktioner osv inte infogas,
eftersom det kan leda till att optimeringen inte terminerar. Detta kan hända om optimeringen körs på alla grenar
i \kw{case}-uttryck. Ett exempel är funktionen \ic{sum}, som summerar en heltalslista:

\begin{codeEx}
sum xs = case xs of
    { Nil -> 0
    ; Cons y ys -> let
        { a = THUNK (sum ys)
        } in y + a
    };
\end{codeEx}

Om man optimerar \ic{sum} inuti grenarna så finns det risk att \ic{sum} infogas
igen i \ic{Cons}-fallet.

\paragraph{ Val av rätt gren i \kw{case}-uttryck }

I \kw{case}-uttryck är ibland granskarens värde känt. Detta kan användas
för att välja rätt gren. När optimeringen kommer till en \kw{case}-sats optimeras först granskaren,
men eftersom det inte är avgörbart på förhand om en optimering lyckas eller inte
behöver det finnas ett sätt att visa det. 

I vår implementation har vi valt att visa detta genom att ha tre olika tillstånd
för optimeraren. Vilka dessa är och vad de betyder kommer att beskrivas senare.

\paragraph{ Utbyte av uttryck mot semantiskt ekvivalenta uttryck}

STG-språket följer vissa lagar, som kan användas för att skriva om uttryck
till andra ekvivalenta uttryck. I vissa fall används dessa lagar för att skriva om
uttryck till något som är lättare att optimera.

\paragraph{}

Då optimeringen kan misslyckas med att ge ett värde behöver uttrycken kunna byggas upp
igen. För att veta vad som skall byggas upp läggs continuations på stacken
som berättar vilket uttryck som tidigare optimerades och som ska byggas upp igen om det
inte blir ett värde. Många uttryck får en egen continuation för att kunna göra denna
uppbyggnad igen.



Som lösning har vi valt att använda tre olika tillstånd under optimeringen. Dessa har vi valt att kalla
$\Omega, \Psi$ och $\Phi$. De används för att optimeringen skall veta vilket skede den
befinner sig i. 

\paragraph{Omega $\Omega$} Detta tillstånd befinner sig maskinen i när den vill optimera något till ett värde. Här
inspekteras uttrycket och evalueras till ett värde om det är möjligt.

\paragraph{Psi $\Psi$} Detta är maskinens tillstånd när den har optimerat något till ett värde. I detta tillstånd
försöker maskinen bygga upp uttrycket igen utifrån stacken, med vetskapen om att den har
något som är ett värde.

\paragraph{Irr $\Phi$} Detta tillstånd kommer maskinen till om den har misslyckats att optimera till ett värde. Irr är alltså kortform för Irreducible.
I detta tillstånd fanns det inte tillräckligt med information att få ett
värde, så uttrycket byggs upp igen med information från stacken.

\paragraph{}
I figur \ref{fig:Optimise:states} ses att evaluering alltid börjar i den vanliga maskinen
 (visas som STG) och går till $\Omega$. Detta sker vid optimering av en funktion.
När optimeringen är klar bygger $\Phi$ upp funktionen igen och går tillbaka till den vanliga maskinen.

%\begin{figure}[H]
\stgOptimise
%\caption{Hur tillstånden kan gå mellan varandra}
%\label{fig:Optimise:states}
%\end{figure}

Figuren visar också att optimeringstillstånden använder sig av varandra, vilket
händer beroende på uttrycket som maskinen har när den kommer till tillståndet.

I resterande kapitel kommer två olika semantiker som båda bygger på dessa tillstånd
att presenteras. Den första är CBV-semantiken som har
problem med att den inte var tillräckligt lat vilket gjorde att många program inte 
kunde optimeras, då optimeringen gick in i oändliga loopar. Senare i sektion \ref{sec:Optimise:CBN} 
kommer CBN-semantiken att presenteras som är vår lösning på det problemet.






\subfile{Optimise/CBV}

\subfile{Optimise/With}

\subfile{Optimise/CBN}

\subfile{Optimise/Vidare}
%\subfile{Optimise/DeadCode}
%\subfile{Optimise/ValkandCaseLag}
%\subfile{Optimise/AfterBurner}

\end{document}
